---
layout: post
title:  "Predicting S&P 500 Returns Based on Daily Stock Market Data"
date:   2021-02-25 21:42:18 -0800
categories: data science project
---



## Introduction
The present analysis takes inspiration from the statistical learning exercise done on the *Weekly* stock market data provided from the book *Introduction to Statistical Learning* by Witten, Tibshirani, Hastie, and M. James. The authors aimed to compare the performances of different statistical learning models trained to classify whether the returns of the S&P 500 will go up or down on a given week based on returns from the previous 5 weeks.   

On this analysis, I extend the methods in the book by using a similar, but more recent dataset of the S&P 500 stock prices provided from ___. It features trading prices from the SPDR S&P 500 ETF captured at the opening and closing of the stock market, as well as at the maximum and minimum trading prices throughout each day from February 25th, 2005 to November 10th, 2017. Moreover, the dataset contains the trading volume, which indicates how many times the SPDR S&P 500 ETF has been traded on each given day.

## Data cleaning 
```r
spy.us <- read.csv("~/Projects/stock market project/spy.us.txt")      # load the dataset

# Use open and close to create percentage returns for the present day.
spy.us['today'] <- round(spy.us$Close / spy.us$Open * 100 - 100, 3)

# Create lags that indicate the previous days' percentage return.
# When this is done, the first 5 days of the dataset will not be used as
# response variables.
spy.us['lag1'] <- c(NA, spy.us$today[-3201]) 
spy.us['lag2'] <- c(NA, spy.us$lag1[-3201]) 
spy.us['lag3'] <- c(NA, spy.us$lag2[-3201]) 
spy.us['lag4'] <- c(NA, spy.us$lag3[-3201]) 
spy.us['lag5'] <- c(NA, spy.us$lag4[-3201]) 

spy.us['volume.lag1'] <- c(NA, spy.us$Volume[-3201]) 
spy.us['volume.lag2'] <- c(NA, spy.us$volume.lag1[-3201)

# clear up dataset to include only relevant details.
spy.us['Year'] <- as.numeric(substring(spy.us$Date, 1, 4))

### reorder the columns
col_order <- c("Year", "lag1", "lag2", "lag3", "lag4", "lag5", 
               "volume.lag1", "volume.lag2", "Volume",
               "today", "Date", "Open", "High", "Low", "Close", "OpenInt")
spy.us <- spy.us[, col_order]

### get rid of less relevant variables
spy.us <- spy.us[6:3201, 1:10]
### reset index
rownames(spy.us) <- NULL 

### add outcome variable
spy.us['direction'] <- rep("down", nrow(spy.us))
spy.us$direction[spy.us$today >= 0] <- "up"
```
## Data Exploration 
```r
# create visualizations
ggscatmat(spy.us, color = "direction")
# no correlations have been observed except between volume and the percentage
# return today, which shows a moderate positive correlation

ggplot(spy.us, aes(today, Volume)) +
  geom_point() + 
  geom_smooth(se = FALSE)

```
![Scatterplot matrix for S&P 500 daily return data](/assets/images/scatterplotmatrix.png)

whats interesting from here is that returns that are either very high or very low are associated with high volumes of exchange 

## Model building

### Logistic Regression
```r
# create models
spy.us$up <- 0
spy.us$up[spy.us$direction == 'up'] <- 1
logreg.fit <- glm(up∼lag1+lag2+lag3+lag4+lag5+Volume,
               data=spy.us ,family=binomial )
summary(logreg.fit)

# volume, lag1, and lag2 are signifant. lets see the training error rate
logreg.probs <- predict (logreg.fit,type="response")
logreg.pred <- rep('down', nrow(spy.us))
logreg.pred[logreg.probs >.5] <- 'up'
table(logreg.pred, spy.us$direction)
mean(logreg.pred == spy.us$direction)
# when the actual direction is up, the model predicts correctlty 87% of the time
# when the actual direction is down, the model predicts correctly 19% of the time
# the error rate is 43.9%

# lets try the same but with only the significant variables and 
# use cross validation to get the test error rather than the training error.

logreg.fit <- glm(up∼lag1+lag2+Volume+volume.lag1+volume.lag2+
                  volume.lag1:volume.lag2+Volume:volume.lag1, 
                  data=spy.us ,family=binomial)
cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)
cv.error <- cv.glm(spy.us,logreg.fit, cost, K=5)$delta[1]
summary(logreg.fit)
# the classification error rate is 44%
```

### Linear Discriminant Analysis (LDA)
```r
# lets try lda with 10-fold cross validation
spy_shuffled <- spy.us[sample(nrow(spy.us)),]
folds <- cut(seq(1,nrow(spy_shuffled)),breaks=10,labels=FALSE)

lda.cv.error <- rep(0, 10)
for(i in 1:10){
  #Segment your data by fold using the which() function 
  test <- which(folds==i,arr.ind=TRUE)
  testData <- spy_shuffled[test, ]
  train <- which(folds!=i,arr.ind=TRUE)
  lda.fit <- lda(up∼lag1+lag2+Volume+volume.lag1+volume.lag2, data = spy_shuffled, subset = train)
  lda.pred <- predict(lda.fit, testData)
  lda.cv.error[i] <- mean(lda.pred$class != testData$up)
}

mean(lda.cv.error)
# the test error rate is 43.6%
```

### Quadratic Discriminant Analysis (QDA)
```r
qda.cv.error <- rep(0, 10)
for(i in 1:10){
  #Segment your data by fold using the which() function 
  test <- which(folds==i,arr.ind=TRUE)
  testData <- spy_shuffled[test, ]
  train <- which(folds!=i,arr.ind=TRUE)
  qda.fit <- qda(up∼lag1+lag2+Volume, data = spy_shuffled, subset = train)
  qda.pred <- predict(qda.fit, testData)
  qda.cv.error[i] <- mean(qda.pred$class != testData$up)
}

mean(qda.cv.error)
# the test error rate is 44.15%
```
## Conclusion
